{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6sSE2Yr0OeJrsGUSTj3RJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashmitha22/ML_projects/blob/main/Deep_learningipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPIlJMfnFvsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25qE8uloFhGf",
        "outputId": "ec06eb3a-2d1f-4c04-ace5-d8e32b338f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---2. Dot Product---\n",
            "Dot product: 32\n",
            "\n",
            "---3. Neuron Example---\n",
            "Neuron output: [[1.9000001]]\n",
            "Activated output: [[1.9000001]]\n",
            "\n",
            "---4. Batch Processing----\n",
            "Batch output:\n",
            " [[1.9000001]\n",
            " [3.7      ]]\n",
            "\n",
            "---5. Softmax---\n",
            "Probabilities:\n",
            " [[0.09003057 0.24472848 0.66524094]]\n",
            "\n",
            "---6.Loss Function---\n",
            "Cross-entropy Loss: [1.4076059]\n",
            "\n",
            "---7. Gradients---\n",
            "Gradient: 8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n---2. Dot Product---\")\n",
        "x=tf.constant([1,2,3])\n",
        "y=tf.constant([4,5,6])\n",
        "dot=tf.tensordot(x,y,axes=1)\n",
        "print(\"Dot product:\",dot.numpy())\n",
        "\n",
        "print(\"\\n---3. Neuron Example---\")\n",
        "X=tf.constant([[1.0,2.0,3.0]])\n",
        "W=tf.constant([[0.1],[0.2],[0.3]])\n",
        "b=tf.constant([0.5])\n",
        "z=tf.matmul(X,W)+b\n",
        "print(\"Neuron output:\",z.numpy())\n",
        "output=tf.nn.relu(z)\n",
        "print(\"Activated output:\",output.numpy())\n",
        "\n",
        "print(\"\\n---4. Batch Processing----\")\n",
        "X_batch= tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
        "z_batch=tf.matmul(X_batch,W)+b\n",
        "print(\"Batch output:\\n\",z_batch.numpy())\n",
        "\n",
        "print(\"\\n---5. Softmax---\")\n",
        "logits=tf.constant([[1.0,2.0,3.0]])\n",
        "probs=tf.nn.softmax(logits)\n",
        "print(\"Probabilities:\\n\",probs.numpy())\n",
        "\n",
        "print(\"\\n---6.Loss Function---\")\n",
        "y_true = tf.constant([[0, 1, 0]]) # Corrected: Reshape to have a batch dimension\n",
        "y_pred = probs # Use the output from the softmax layer directly\n",
        "loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "print(\"Cross-entropy Loss:\", loss.numpy())\n",
        "\n",
        "print(\"\\n---7. Gradients---\")\n",
        "x_var = tf.Variable(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x_var**2 + 2*x_var + 1\n",
        "dy_dx = tape.gradient(y, x_var)\n",
        "print(\"Gradient:\", dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQK4rSzkFiWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}